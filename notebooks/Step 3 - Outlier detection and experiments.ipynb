{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from protodata.display import *\n",
    "from protodata.data_ops import *\n",
    "from protodata.utils import *\n",
    "from protodata.datasets import Datasets\n",
    "from protodata.datasets.airbnb import get_data_path, get_amenities_path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "\n",
    "import warnings\n",
    "\n",
    "%matplotlib inline\n",
    "pandas.set_option(\"display.max_columns\", 100)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data and finding duplicates\n",
    "\n",
    "First we read the created dataset and look for possible duplicates. Since we did not collect the data ourselves, we cannot ensure that data does not contain duplicities.\n",
    "\n",
    "We discard those entries which have repeated listing identifiers and, as a matter of understanding, we also want to know how many listings have duplicated images in their profiles. Sometimes users offer more than one lodging with similar characteristics (not identical) and post the same pictures for all of them. Nevertheless, the amount of repeated image links is very low (~0.03%) and preserving will not do any harm in future steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "airbnb_root = get_tmp_data_location(Datasets.AIRBNB_PRICE)\n",
    "data = pd.read_csv(get_data_path(airbnb_root))\n",
    "metadata = load_pickle(get_amenities_path(airbnb_root))\n",
    "\n",
    "# Duplicated ids\n",
    "duplicated_id = data.duplicated('id', keep=False)\n",
    "print('Found %d duplicated ids. Removing them ...' % duplicated_id[duplicated_id == True].shape[0])\n",
    "data = data.drop(data[duplicated_id == True].index)\n",
    "\n",
    "# Duplicated urls\n",
    "duplicated_pic = data.duplicated('picture_url', keep=False)\n",
    "print('Found %d duplicated picture urls. Removing them...' % duplicated_pic[duplicated_pic == True].shape[0])\n",
    "data = data.drop(data[duplicated_pic == True].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [x for x in data.columns.values]\n",
    "print('List of columns: {}'.format(col_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column overview\n",
    "\n",
    "Let's have a look at the columns to see if data is in the expected range. First let's observe the numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary columns\n",
    "\n",
    "We have some columns which are intended to be binary but have a string representation ('f'/'t'). Let's binarize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def str_to_binary(x):\n",
    "    \"\"\" Returns binary value of the input word \"\"\"\n",
    "    if x.lower() == 't':\n",
    "        return True\n",
    "    elif x.lower() == 'f':\n",
    "        return False\n",
    "    else:\n",
    "        raise ValueError('Unexpected feature value {}'.format(x))\n",
    "\n",
    "to_binarize = ['host_has_profile_pic', 'host_identity_verified', 'host_is_superhost', 'instant_bookable']\n",
    "for c in to_binarize:\n",
    "    data.loc[:, c] = data[c].apply(str_to_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those binary columns will be treated as numeric values in the future. Therefore, we can convert them into floats. From now and on, these columns will be considered as numerical features. Amenities, however, will be treated as sparse columns in the future and we prefer to keep them like boolean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = convert_boolean(data, excluded_columns=list(metadata['amenities']), func=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the histogram of the numeric columns. First we separate columns by their nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate between numeric and categorical columns\n",
    "num_cols = data._get_numeric_data().columns\n",
    "cat_cols = list(set(data.columns.values) - set(num_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot histograms of numerical data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numeric_data = data[num_cols]\n",
    "excluded_num = ['id', 'scrape_id']\n",
    "plot_histograms(data=numeric_data, \n",
    "                var=[c for c in num_cols if c not in excluded_num], \n",
    "                path=None, \n",
    "                prefix=None, \n",
    "                nrows=2, \n",
    "                grid=4, \n",
    "                bins=25,\n",
    "                fonts=8, \n",
    "                fig_size=(10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Taking a quick glance at the column descriptions, we can observe:\n",
    "\n",
    "* Most of the hosts have profile picture while just 2 thirds are verified. Just a few of them are superhosts.\n",
    "* Accommodates, beds, bedrooms and bathrooms follow a reasonable distribution. We may find options with no bedrooms (e.g. studio, cabin) or no bathrooms, but it is not expected not to have a bed (even if the bed is not a traditional one).\n",
    "* Cleaning fee seems to have outliers. We observe the same for the security deposit.\n",
    "* Prices also seem to include very high values that we will consider as outliers.\n",
    "* Number of maximum guests (accommodates) are mainly distributed between 0 and 6, but there are also lodgings offering up to 18 guests, which is a reasonable amount for large apartments.\n",
    "* Host verifications range from 1 to 10, which is ok.\n",
    "* Minimum number of nights also happen to contain outliers.\n",
    "* Though there are some listings with very high number of reviews per month (up to 25), we regard this as a rare but feasible situation.\n",
    "* Amenities seem to be reasonably distributed.\n",
    "* Review columns have negative values when missing (non-rated). We see that listings are, on average, rated high (above 8 for specific scores and above 60 for overall rating).\n",
    "* The price for extra people contain also high values (outliers).\n",
    "* We see there are a high amount of listings that are either fully booked or barely busy.\n",
    "* There are some listings which include no guests, which does not have non-sense.\n",
    "\n",
    "Let's further observe those columns which seemed to contain outliers in the visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Beds\n",
    "\n",
    "We discard those instances that do not have any beds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_before = data.shape[0]\n",
    "data = data[data['beds'] >= 1]\n",
    "size_after = data.shape[0]\n",
    "print('Eliminated %d instances out of %d after removing rows without beds'\n",
    "     % (size_before - size_after, size_before))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning fee\n",
    "\n",
    "We just consider lodgings that have cleaning fees below 750 dollars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_before = data.shape[0]\n",
    "data = data[data['cleaning_fee'] <= 500]\n",
    "size_after = data.shape[0]\n",
    "print('Eliminated %d instances out of %d after removing cleaning fees above 750 dollars'\n",
    "     % (size_before - size_after, size_before))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Price\n",
    "\n",
    "We saw that the 75 percentile of the prices is around 170 dollars while the maximum price recorded is above 300k dollars. It is also noticeable that there are prices around 0 that must be omitted. We decide to discard those prices below 20 dollars and above 1500. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_before = data.shape[0]\n",
    "data = data[(data['final_price'] <= 1500) & (data['final_price'] >= 20)]\n",
    "size_after = data.shape[0]\n",
    "print('Eliminated %d instances out of %d after removing prices below 20 dollars and above 1500'\n",
    "     % (size_before - size_after, size_before))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum nights\n",
    "\n",
    "Since our project will focus on short-term lodging price prediction, we will restrict lodgings that have more than 30 days as minimum stay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "size_before = data.shape[0]\n",
    "data = data[data['minimum_nights'] <= 30]\n",
    "size_after = data.shape[0]\n",
    "print('Eliminated %d instances out of %d after removing minimum stays above 30 days'\n",
    "     % (size_before - size_after, size_before))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Security deposit\n",
    "\n",
    "Let's consider a maximum of 2000 dollars as security deposit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_before = data.shape[0]\n",
    "data = data[data['security_deposit'] <= 2000]\n",
    "size_after = data.shape[0]\n",
    "print('Eliminated %d instances out of %d after removing security deposits above 2000 dollars'\n",
    "     % (size_before - size_after, size_before))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guests included\n",
    "\n",
    "Guests are the number of people that are included in the price. From that number to the maximum number (accommodates), an extra price (extra_people) is paid per person and night.\n",
    "\n",
    "First, let's ensure that the number of guests never surpasses the number of accommodates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_before = data.shape[0]\n",
    "data = data[data['guests_included'] <= data['accommodates']]\n",
    "size_after = data.shape[0]\n",
    "print('Eliminated %d instances out of %d after removing non-coherent guests feature'\n",
    "     % (size_before - size_after, size_before))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of guests is the number of people the host includes in the price. Therefore, let's discard those entries containing no guests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_before = data.shape[0]\n",
    "data = data[data['guests_included'] > 0]\n",
    "size_after = data.shape[0]\n",
    "print('Eliminated %d instances out of %d after removing entries without guests included'\n",
    "     % (size_before - size_after, size_before))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra people\n",
    "\n",
    "The extra_people column contains the number of dollars to be paid for extra guests not included in the price.\n",
    "Extra people price is not expected to be above the base price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_before = data.shape[0]\n",
    "data = data[data['final_price'] >= data['extra_people']]\n",
    "size_after = data.shape[0]\n",
    "print('Eliminated %d instances out of %d after removing high extra people prices'\n",
    "     % (size_before - size_after, size_before))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores\n",
    "\n",
    "We have seen that most of the scores are distributes in a similar way: most of the values are top-scores or non-rated while 9 scores have considerable support and the other options have a very limited support. We decide to define 4 categories: very good (score of 10), good (score of 9), regular (less than 9) and non-rated for all scores between 0 and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categorize_score(score):\n",
    "    \"\"\" Converts score into category considering the input bounds. Categories defined as: \n",
    "            - 10 is very good\n",
    "            - 9 is good\n",
    "            - below 9 is regular\n",
    "            - Missing is non-rated\n",
    "    \"\"\"\n",
    "    if score == -1:\n",
    "        return \"non-rated\"\n",
    "    else:\n",
    "        if score == 10:\n",
    "            return \"very good\"\n",
    "        elif score == 9:\n",
    "            return \"good\"\n",
    "        elif score < 9:\n",
    "            return \"regular\"\n",
    "\n",
    "# Categorize scores in interval 0-10\n",
    "review_10_scores = ['review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin',\n",
    "    'review_scores_communication', 'review_scores_location', 'review_scores_value']\n",
    "for c in review_10_scores:\n",
    "    data.loc[:, c] = data[c].apply(categorize_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score rating\n",
    "\n",
    "Let's now categorize the general score of the listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categorize_rating(score):\n",
    "    \"\"\" Converts rating into category considering the input bounds. Categories defined as: \n",
    "            - 95 < x <= 100: very good\n",
    "            - 90 <= x <= 95: good\n",
    "            - x < 90: regular\n",
    "            - Missing is non-rated\n",
    "    \"\"\"\n",
    "    if score == -1:\n",
    "        return \"non-rated\"\n",
    "    else:\n",
    "        if score > 95:\n",
    "            return \"very good\"\n",
    "        elif score >= 90 and score <= 95:\n",
    "            return \"good\"\n",
    "        elif score < 90:\n",
    "            return \"regular\"\n",
    "\n",
    "col = 'review_scores_rating'\n",
    "data.loc[:, col] = data[col].apply(categorize_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical columns\n",
    "\n",
    "Now it is time to analyse the categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show_categorical(dataframe, cols, excluded_cols, normalized=False):\n",
    "    \"\"\" Shows distribution of categories within each categorical feature \"\"\"\n",
    "    subset = dataframe[cols]\n",
    "    for c in subset:\n",
    "        if c not in excluded_cols:\n",
    "            print('Counts for %s' % c)\n",
    "            unique = subset[c].value_counts(normalize=normalized)\n",
    "            print(unique)\n",
    "            print('\\n')\n",
    "\n",
    "excluded_cat = ['listing_url', 'picture_url', 'last_scraped']\n",
    "show_categorical(data, cat_cols, excluded_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing small cities\n",
    "\n",
    "We observe that there are many cities with just a bunch of listings, which add sparsity to the resulting dataset without adding much information. Let's just use cities with more than 5000 listings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts = data['area'].value_counts(normalize=False)\n",
    "big_cities = city_counts[city_counts > 5000].index\n",
    "data = data[data['area'].isin(big_cities)]\n",
    "print('List of resulting cities({}): {}'.format(len(big_cities), data['area'].unique().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see now how data categorical data is distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_categorical(data, cat_cols, excluded_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing sparsity\n",
    "\n",
    "There are many identifiers that are very little supported in the data. As an example, we can see how no_refunds, long_term or super strict cancelation policies have very few instances. We are going to merge those categories in columns to represent a new category which is already not represented by the other ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aggregate_categories(data, column, minimum_support, new_category):\n",
    "    \"\"\" Converts those categories with support below threshold into a new category \"\"\"\n",
    "    counts = data[column].value_counts(normalize=True)\n",
    "    values = counts[counts < minimum_support].index\n",
    "    data.loc[data[column].isin(values), column] = new_category\n",
    "    return data\n",
    "\n",
    "data = aggregate_categories(data, 'cancellation_policy', 0.001, 'other')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for the property types, discarding those below 200 instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = aggregate_categories(data, 'property_type', 0.001, 'Other')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we have should apply this logic for the subareas (neighbourhood). Let's keep those who have at least 0.01% of instances and set the rest to \"Other\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subareas_before = len(data['subarea'].unique())\n",
    "data = aggregate_categories(data, 'subarea', 0.0001, 'Other')\n",
    "subareas_after = len(data['subarea'].unique())\n",
    "print('Before we had %d neighbourhoods and now we have %d' % (subareas_before, subareas_after))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before saving the final version of the dataset, let's have another general look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_categorical(data, cat_cols, excluded_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's visualize the distribution of the prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['final_price'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the anual availability of the listings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['availability_365'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['final_price'].head(n=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we save the final version of the dataset into a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final dataset contains %d instances and %d columns' % (data.shape[0], data.shape[1]))\n",
    "data.to_csv(get_data_path(airbnb_root), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

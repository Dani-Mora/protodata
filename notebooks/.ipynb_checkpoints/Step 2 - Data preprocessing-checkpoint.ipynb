{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset construction\n",
    "\n",
    "This notebook assumes that Step 1 has been executed and that all data has been downloaded already. If not, proceed to execute that notebook before going on. \n",
    "\n",
    "In this notebook we create an additional csv with the clean data and a general dataset gathering all instances from all cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dataio.utils import *\n",
    "from dataio.data_ops import *\n",
    "from dataio.datasets import Datasets\n",
    "from dataio.datasets.airbnb import get_amenities_path, get_data_path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from currency_converter import CurrencyConverter\n",
    "import warnings\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RELEVANT_COLUMNS =  ['accommodates', 'area', 'bathrooms', 'bed_type', 'bedrooms', 'beds', 'cancellation_policy', \n",
    "        'cleaning_fee', 'country', 'final_price', 'guests_included', \n",
    "        'host_has_profile_pic', 'host_identity_verified', 'host_is_superhost', 'host_verifications', \n",
    "        'id', 'instant_bookable', 'last_scraped', 'listing_url', 'picture_url', 'minimum_nights', \n",
    "        'property_type', 'recent_review', 'review_scores_accuracy', 'reviews_per_month',\n",
    "        'review_scores_checkin', 'review_scores_cleanliness', 'review_scores_communication', \n",
    "        'review_scores_location', 'review_scores_rating', 'review_scores_value', 'room_type', \n",
    "        'scrape_id', 'security_deposit', 'state', 'subarea', 'availability_365', 'extra_people']\n",
    "\n",
    "\n",
    "def missing_ratio(data, col):\n",
    "    \"\"\" Returns the missing ratio of a column in the input DataFrame \"\"\"\n",
    "    return sum(data[col].isnull())/float(data.shape[0])\n",
    "\n",
    "\n",
    "def get_missings(data, ratio=0.10):\n",
    "    \"\"\" Returns those columns which have more than ratio % of missing instances \"\"\"\n",
    "    missings = []\n",
    "    for i in data.columns:\n",
    "        current = missing_ratio(data, i)\n",
    "        if current > ratio:\n",
    "            missings.append((i, current))\n",
    "    return missings\n",
    "\n",
    "\n",
    "def check_missings(data, min_ratio, max_ratio):\n",
    "    \"\"\" Checks that columns have a reasonable amount of missing values. Min_ratio is the \n",
    "    minimum ratio of missings to consider a column with missings while max_ratio is the minimum\n",
    "    missing ratio to raise an error for columns which tend to have high number of missings \"\"\"\n",
    "    for (n, r) in get_missings(data, ratio=min_ratio):\n",
    "        if n in RELEVANT_COLUMNS:\n",
    "            # Column of interest\n",
    "            if n.startswith(\"review\"):\n",
    "                # Reviews usually have a high amount of missing values (non-rated apartments)\n",
    "                # but we do not expect to be dominant either\n",
    "                if r > max_ratio:\n",
    "                    raise RuntimeError('Column %s has unexpected ratio of missings %f' % (n, r))\n",
    "            elif n not in ['cleaning_fee', 'security_deposit']:\n",
    "                # Deposits can have high number of missings (which translates into 0) but\n",
    "                # rest of columns must have less than min_ratio ratio of missings\n",
    "                raise RuntimeError('Column %s has unexpected ratio of missings %f' % (n, r))\n",
    "                    \n",
    "        \n",
    "def parse_price(x, parsed_time, converter):\n",
    "    \"\"\" Parse price into dollars (numeric) \"\"\"\n",
    "    \n",
    "    def parse_amount(x):\n",
    "        \"\"\" Parse string formatted prince into numeric \"\"\"\n",
    "        return float(x[1:].replace(',', ''))\n",
    "    \n",
    "    if type(x) == float:\n",
    "        return x\n",
    "    else:\n",
    "        x = x.strip() # Eliminate possible blankspaces at end and beggining\n",
    "        if x[0] == '$': # $\n",
    "            return parse_amount(x)\n",
    "        elif x[0] == u\"\\xA3\": # £\n",
    "            return converter.convert(parse_amount(x), 'GBP', 'USD', date=parsed_time)\n",
    "        elif x[0] == u\"\\u20ac\": # €\n",
    "            return converter.convert(parse_amount(x), 'EUR', 'USD', date=parsed_time)\n",
    "        else:\n",
    "            raise ValueError('Unkown currency %s' % x[0])\n",
    "\n",
    "\n",
    "def get_price(mean_price, default_price):\n",
    "    \"\"\" Returns the price of the lodgning as the mean anual price. \n",
    "    If no anual price available, we get the default price in the listings \"\"\"\n",
    "    if np.isnan(mean_price):\n",
    "        return default_price\n",
    "    else:\n",
    "        return mean_price\n",
    "\n",
    "\n",
    "def process_price_column(data, cal, scraped=True):\n",
    "    \"\"\" Processes the column price. If scraped is True, price is taken from the scraped dataset. \n",
    "    Otherwise it is regarded as the mean price through the recorded interval, when available. \n",
    "    The resulting price in both cases is stored in a column named 'final_price' \"\"\"\n",
    "    if scraped is True:\n",
    "        print('\\t - Using scraped price')\n",
    "        data['final_price'] = data['price']\n",
    "    else:\n",
    "        print('\\t - Using mean price if available. Otherwise using scraped price')\n",
    "        # Compute mean price during period\n",
    "        mean_price_year = cal.groupby('listing_id').mean().reset_index()\n",
    "        merged = pd.merge(mean_price_year, data, left_on=['listing_id'], right_on=['id'])\n",
    "        # Get price as the average if available. Otherwise get the scraped one\n",
    "        merged['final_price'] = merged.apply(lambda x: get_price(x['price_x'], x['price_y']), axis=1)\n",
    "        merged = merged[['final_price', 'listing_id']]\n",
    "        # Merge final price into the listings\n",
    "        data = pd.merge(data, merged, left_on=['id'], right_on=['listing_id'])\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def process_prices(data, cal, conv):\n",
    "    \"\"\" Converts prices in both DataFrames into numeric format in dollar currency \"\"\"\n",
    "    # Convert dates involved into proper format        \n",
    "    data['last_scraped'] = data['last_scraped'].apply(to_date)\n",
    "    cal['date'] = cal['date'].apply(to_date)\n",
    "\n",
    "    # Convert prices to dollars\n",
    "    data['extra_people'] = data.apply(lambda x: parse_price(x['extra_people'], x['last_scraped'], conv), axis=1)\n",
    "    data['price'] = data.apply(lambda x: parse_price(x['price'], x['last_scraped'], conv), axis=1)\n",
    "    cal['price'] = cal.apply(lambda x: parse_price(x['price'], x['date'], conv), axis=1)\n",
    "    data['security_deposit'] = data.apply(lambda x: parse_price(x['security_deposit'], x['last_scraped'], conv),\n",
    "                                          axis=1)\n",
    "    data['cleaning_fee'] = data.apply(lambda x: parse_price(x['cleaning_fee'], x['last_scraped'], conv),\n",
    "                                      axis=1)\n",
    "    return data, cal                  \n",
    "\n",
    "\n",
    "def process_neighbourhood(data, neighs):\n",
    "    \"\"\" Places into 'neighbourhood' the neighbourhood information of each row given \n",
    "    the read geojson data. Sets NaN if coordinates are not enclosed into any defined neighbourhood \"\"\"\n",
    "\n",
    "    def get_neigh(longitude, latitude):\n",
    "        \"\"\" Returns the neighbourhood of the input coordinates\"\"\"\n",
    "        retrieved = get_neighborhood(neighs, longitude, latitude)\n",
    "        return np.nan if retrieved is None else unicode_to_str(retrieved)\n",
    "    \n",
    "    data['subarea'] = data.apply(lambda x: get_neigh(x['longitude'], x['latitude']), axis=1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def parse_amenities(x):\n",
    "    \"\"\" Parses amenities string format into a set \"\"\"\n",
    "    \n",
    "    if x[0] == '{':\n",
    "        x = x[1:]\n",
    "    if x[-1] == '}':\n",
    "        x = x[:-1]\n",
    "    \n",
    "    def parse_elem(elem):\n",
    "        \"\"\" Parses each element between colons \"\"\"\n",
    "        if elem[0] == '\"':\n",
    "            elem = elem[1:]\n",
    "        if elem[-1] == '\"':\n",
    "            elem = elem[:-1]\n",
    "        return elem.lower()\n",
    "\n",
    "    if not x:\n",
    "        # Empty list\n",
    "        return []\n",
    "    else:\n",
    "        return set([parse_elem(elem) for elem in x.split(\",\")])\n",
    "\n",
    "    \n",
    "def get_amenity_bool(am_list, am):\n",
    "    \"\"\" Whether the input amenity is included in the input amenity list \"\"\"\n",
    "    return am in parse_amenities(am_list)\n",
    "\n",
    "                           \n",
    "def check_amenities(data):\n",
    "    \"\"\" Returns copy of the input DataFrame where rows containing empty amenities have been removed \"\"\"\n",
    "    # Get set of amenities\n",
    "    empty = []\n",
    "    for index, row in data.iterrows():\n",
    "        # Parse amenities for current row\n",
    "        current_amenities = parse_amenities(row['amenities'])\n",
    "        # If no amenity found, track it\n",
    "        if len(current_amenities) == 0:\n",
    "            empty.append(index)\n",
    "\n",
    "    # Erase those which have no amenities (all lodgings should have at least one)\n",
    "    print('\\t - Detected %d listings with no amenities' % (len(empty)))\n",
    "    return data             \n",
    "\n",
    "\n",
    "def create_recent_review(inp_date, scraped_date, recent_thresh):\n",
    "    \"\"\" Returns whether it has a recent review. Assumes scraped date is never null \"\"\"\n",
    "    if type(inp_date) == float and np.isnan(inp_date):\n",
    "        return False\n",
    "    elif type(inp_date) == str:\n",
    "        return (scraped_date - to_date(inp_date)).days <= recent_thresh\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected date format {}\".format(inp_date))\n",
    "        \n",
    "\n",
    "def read_data(path):\n",
    "    \"\"\" Read the 3 associates files for the dataset pointed by the input path \"\"\"\n",
    "    ratings = pd.read_csv(os.path.join(path, 'reviews.csv'))\n",
    "    listings = pd.read_csv(os.path.join(path, 'listings.csv'))\n",
    "    calendar = pd.read_csv(os.path.join(path, 'calendar.csv'))\n",
    "    neighs = read_city_data(os.path.join(path, 'neighbourhoods.geojson'))\n",
    "    meta = load_pickle(os.path.join(path, 'info.dat'))\n",
    "    return ratings, listings, calendar, neighs, meta\n",
    "\n",
    "\n",
    "def count_verifications(x):\n",
    "    \"\"\" Parses the verifications list and returns the number of verifications of the host \"\"\"\n",
    "    return len(x[1:-1].split(','))\n",
    "\n",
    "\n",
    "def handle_missing(data):\n",
    "    \"\"\" Handles missing data with specific actions depending on the column content \"\"\"\n",
    "    # Handle missings in security and cleaning fee\n",
    "    data.loc[data['cleaning_fee'].isnull(), 'cleaning_fee'] = 0.0\n",
    "    data.loc[data['security_deposit'].isnull(), 'security_deposit'] = 0.0\n",
    "    \n",
    "    # Check to false all those host_is_superhost information\n",
    "    data.loc[data['host_is_superhost'].isnull(), 'host_is_superhost'] = 'f'\n",
    "    \n",
    "    # Set missings in any type of review to -1 (no score means not rated yet)\n",
    "    reviews = ['review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin',\n",
    "                'review_scores_communication', 'review_scores_location', 'review_scores_value', \n",
    "               'review_scores_rating']\n",
    "    for c in reviews:\n",
    "        data.loc[data[c].isnull(), c] = -1\n",
    "\n",
    "    # Drop columns independently to see which columns tend to be missing\n",
    "    for c in data.columns.values:\n",
    "        data_before = data.shape[0]\n",
    "        data = data.dropna(subset=[c])\n",
    "        diff = data_before - data.shape[0]\n",
    "        if diff > 0:\n",
    "            print('\\t - Erased %d instances for missings in %s' % (diff, c))\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def review_to_num(x):\n",
    "    \"\"\" Parses monthly reviews to numeric so nan's are converted to zeros \"\"\"\n",
    "    if np.isnan(x):\n",
    "        return 0\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "    \n",
    "def set_geographical(data, meta):\n",
    "    \"\"\" Sets the city, state, country and data for all rows \"\"\"\n",
    "    data['state'] = unicode_to_str(meta['state'])\n",
    "    data['country'] = unicode_to_str(meta['country'])\n",
    "    data['area'] = unicode_to_str(meta['city'])\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_dataset(data_root, amenities_list, scraped_price=True, min_ratio=0.05, max_ratio=0.55):\n",
    "    \"\"\" Returns the DataFrame corresponding to the dataset contianed in the input folder\n",
    "    Args:\n",
    "        data_root: Dataset root folder for the particular city\n",
    "        amenities_list: Amenities to use as columns in the data\n",
    "        scraped_price: Whether to use the scraped price (True) or the mean price during \n",
    "            the recorded year (False). If False and mean price not available, the scraped one is used.\n",
    "        min_ratio: Minimum ratio at which we consider a column to have high number of missing values.\n",
    "            If relevant columns with high missings are detected, an error is raised.\n",
    "        max_ratio: For those columns used which usually have high number of missings, this is the \n",
    "            upper bound for the missings. If surpassed, an error is raised.\n",
    "    Returns:\n",
    "        data_subset: processed data\n",
    "    \"\"\"\n",
    "    ratings, listings, calendar, neighs, meta = read_data(data_root)\n",
    "    \n",
    "    conv = CurrencyConverter() # Used for parsing prices\n",
    "    data_before = listings.shape[0]\n",
    "    \n",
    "    # Set geographical information\n",
    "    listings = set_geographical(listings, meta)\n",
    "    \n",
    "    # Check missing values. Raises error if missing ratio unexpected\n",
    "    check_missings(listings, min_ratio=min_ratio, max_ratio=max_ratio)\n",
    "    \n",
    "    # Set proper lodging area information (new column 'area')\n",
    "    listings = process_neighbourhood(listings, neighs)\n",
    "\n",
    "    # Parse reviews per month so missings are 0.0\n",
    "    listings['reviews_per_month'] = listings['reviews_per_month'].apply(review_to_num)\n",
    "    \n",
    "    # Convert all prices to dollar currency and numeric format\n",
    "    listings, calendar = process_prices(listings, calendar, conv)\n",
    "    \n",
    "    # Process dataset price into a new column 'final_price'\n",
    "    listings = process_price_column(listings, calendar, scraped=scraped_price)\n",
    "\n",
    "    # Host verifications: from list to numeric\n",
    "    listings['host_verifications'] = listings['host_verifications'].apply(count_verifications)\n",
    "    \n",
    "    # Delete instances without amenities and create dummy variables for them\n",
    "    listings = check_amenities(listings)\n",
    "    for a in amenities_list:\n",
    "        listings[a] = listings['amenities'].apply(lambda x: get_amenity_bool(x, a))\n",
    "\n",
    "    # Create a column stating whether the scraped lodging had a recent revied (within 30 days)\n",
    "    listings.loc[:, 'recent_review'] = \\\n",
    "        listings.apply(lambda x: create_recent_review(x['last_review'], x['last_scraped'], 30), axis=1)\n",
    "        \n",
    "    # Build final dataset\n",
    "    data_subset = listings[RELEVANT_COLUMNS + amenities_list]\n",
    "    \n",
    "    # Deal with missing information\n",
    "    data_subset = handle_missing(data_subset)\n",
    "\n",
    "    data_after = data_subset.shape[0]\n",
    "    print('\\t - Apartmens before: %d, after: %d' % (data_before, data_after))\n",
    "    \n",
    "    return data_subset\n",
    "\n",
    "\n",
    "# Debug functions\n",
    "\n",
    "\n",
    "def check_neighbours(data_root, min_instances=100):\n",
    "    \"\"\" Prints the number of neighbourhoods per city that have less than the given instances \"\"\"\n",
    "    \n",
    "    def get_out_of_range(data, column, minimum):\n",
    "        \"\"\" Given a categorical column, check how many of its values have less support than a threshold \"\"\"\n",
    "        if column in data:\n",
    "            counts = data[column].value_counts(normalize=False)\n",
    "            under = counts[counts < minimum]\n",
    "            print('Column %s out of range: %d out of %d' % (column, len(under), counts.shape[0]))\n",
    "\n",
    "    ratings, listings, calendar, neighs, meta = read_data(data_root)\n",
    "    \n",
    "    # Set proper lodging area information (new column 'area')\n",
    "    listings = process_neighbourhood(listings, neighs)\n",
    "\n",
    "    # Count neighbourhood instances\n",
    "    get_out_of_range(listings, 'subarea', min_instances)\n",
    "    get_out_of_range(listings, 'neighbourhood', min_instances)\n",
    "    get_out_of_range(listings, 'neighbourhood_cleansed', min_instances)\n",
    "    get_out_of_range(listings, 'neighbourhood_group_cleansed', min_instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared amenities\n",
    "\n",
    "Before conatenating the data, we must ensure that we use a shared set of amenities (e.g. services, equipments) for all datasets. Though the number of amenities per dataset is above 40, the number of shared ones is slightly less than that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_amenities(data_root):\n",
    "    \"\"\" Returns the set of unique amenities found for the dataset in the input directory \"\"\"\n",
    "    listings = pd.read_csv(os.path.join(data_root, 'listings.csv'))\n",
    "    # Get set of amenities\n",
    "    amenities = set()\n",
    "    for index, row in listings.iterrows():\n",
    "        # Parse amenities for current row\n",
    "        current_amenities = parse_amenities(row['amenities'])\n",
    "        amenities = amenities.union(current_amenities)\n",
    "    return amenities\n",
    "\n",
    "# Get set of unique common ammenities for all datasets considered\n",
    "print('Extracting amenities ...')\n",
    "airbnb_root = get_tmp_data_location(Datasets.AIRBNB_PRICE)\n",
    "common_amenities = None\n",
    "for subf in get_subfolders(airbnb_root):\n",
    "    current_am = get_amenities(os.path.join(airbnb_root, subf))\n",
    "    if common_amenities is None:\n",
    "        common_amenities = current_am\n",
    "    else:\n",
    "        common_amenities = common_amenities.intersection(current_am)\n",
    "\n",
    "print('Found {} common amenities. List: {}'.format(len(common_amenities), common_amenities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the data\n",
    "\n",
    "Now we can proceed to filter the downloaded datasets. For each city, the processed dataset is placed in its respective folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "overwrite = True\n",
    "\n",
    "for subf in get_subfolders(airbnb_root):\n",
    "    \n",
    "    # Get city folder and destination path\n",
    "    city_folder = os.path.join(airbnb_root, subf)\n",
    "    processed_path = os.path.join(city_folder, 'processed.csv') \n",
    "    \n",
    "    if os.path.isfile(processed_path) and overwrite is False:\n",
    "        print('Data already computed for %s. Skipping ...' % subf)\n",
    "    \n",
    "    else:\n",
    "        print('Processing data for %s ...' % subf)\n",
    "        current_data = read_dataset(city_folder, amenities_list=list(common_amenities), scraped_price=False)\n",
    "        current_data.to_csv(processed_path, index=False)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining datasets\n",
    "\n",
    "Finally, we can join all instances from the different cities into a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all processed data frames\n",
    "cities = []\n",
    "for subf in get_subfolders(airbnb_root):\n",
    "    \n",
    "    # Read city data\n",
    "    city_folder = os.path.join(airbnb_root, subf)\n",
    "    processed_path = os.path.join(city_folder, 'processed.csv')\n",
    "    city_data = pd.read_csv(processed_path)\n",
    "    \n",
    "    # Set listing id as data frame id\n",
    "    city_data = city_data.set_index(['id'])\n",
    "    cities.append(city_data)\n",
    "    \n",
    "# Creating one general dataframe\n",
    "final = pd.concat(cities)\n",
    "\n",
    "# Save data and metadata separately. \n",
    "# Could be done together using HDF files but requires extra installation packages, which we want to avoid\n",
    "metadata= {'amenities': common_amenities}\n",
    "save_pickle(get_amenities_path(airbnb_root), metadata)\n",
    "final.to_csv(get_data_path(airbnb_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction\n",
    "\n",
    "This notebook dynamicall downloads Airbnb datasets from the Inside Airbnb site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re, pickle\n",
    "from datetime import datetime\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "import tempfile, shutil, os\n",
    "from dataio.utils import *\n",
    "from dataio.datasets import Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data selection\n",
    "\n",
    "Let's select all those dataset links belonging to the United States."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Match data urls in inseide Airbnb\n",
    "content = requests.get('http://insideairbnb.com/get-the-data.html')\n",
    "matches = re.findall('<a href=\"(.*\\.gz)\"', content.text)  # All cities \n",
    "matches_geo = re.findall('<a href=\"(.*/visualisations/neighbourhoods.geojson)', content.text)  # All demographic info\n",
    "matches = matches + matches_geo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's extract the 3 data files (listing, review, calendar) for each city. Since older versions may also exist, we decide to take the lastes versions available for each city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_geo_info(url):\n",
    "    \"\"\" Returns the country, state and city name, in this order, of the city \"\"\"\n",
    "    split = url.split('/')\n",
    "    return split[3].lower(), split[4].lower(), split[5].lower()\n",
    "\n",
    "def get_data(url):\n",
    "    \"\"\" Returns the kind of data the url belongs to \"\"\"\n",
    "    data = url.split('/')[-1].split('.')[0]\n",
    "    if data not in ['listings', 'calendar', 'reviews', 'neighbourhoods']:\n",
    "        raise ValueError('Unkown data frame found')\n",
    "    return data\n",
    "\n",
    "def get_time(url):\n",
    "    \"\"\" Returns the timestamp associated with the given link\"\"\"\n",
    "    str_date = url.split('/')[6]\n",
    "    return datetime.strptime(str_date, '%Y-%m-%d')\n",
    "\n",
    "\n",
    "# Store URLs for each city encountered\n",
    "datasets = {}\n",
    "for m in matches:\n",
    "    country, state, city = get_geo_info(m)\n",
    "    data, timestamp = get_data(m), get_time(m)\n",
    "    store = True\n",
    "    \n",
    "    if city not in datasets:\n",
    "        datasets[city] = {}\n",
    "    if data in datasets[city] and 'timestamp' in datasets[city]:\n",
    "        if datasets[city]['timestamp'] > timestamp:\n",
    "            # Found older data for the given city, we want the most updated one\n",
    "            print('Found older %s for %s' % (data, city))\n",
    "            store = False\n",
    "        \n",
    "    # Store data if convenient\n",
    "    if store:\n",
    "        datasets[city][data] = m\n",
    "        datasets[city]['country'] = country\n",
    "        datasets[city]['state'] = state\n",
    "        datasets[city]['timestamp'] = timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the links extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in datasets.keys():\n",
    "    print('City %s:' % k)\n",
    "    print('\\t - Listings: %s' % datasets[k]['listings'])\n",
    "    print('\\t - Calendar: %s' % datasets[k]['calendar'])\n",
    "    print('\\t - Reviews: %s' % datasets[k]['reviews'])\n",
    "    print('\\t - Neighbourhoods: %s' % datasets[k]['neighbourhoods'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure data is consistent and get the earlistes and latests date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_same_year(data):\n",
    "    \"\"\" Checks all datasets from cities are from the same year and \n",
    "    returns the earlistes and latest dates\"\"\"\n",
    "    \n",
    "    def overall(l, r, c, func):\n",
    "        return func(l, func(r, c))\n",
    "    \n",
    "    min_date, max_date = None, None\n",
    "    for c in data.keys():\n",
    "        # Get all dates\n",
    "        time_l = get_time(data[c]['listings'])\n",
    "        time_r = get_time(data[c]['reviews'])\n",
    "        time_c = get_time(data[c]['calendar'])\n",
    "        time_n = get_time(data[c]['neighbourhoods'])\n",
    "        \n",
    "        if time_l != time_r or time_l != time_c or time_l != time_n:\n",
    "            raise ValueError('Unconsistent dates for city %s' % c)\n",
    "        \n",
    "        # At this point we know all datasets are form same date\n",
    "        if min_date is None:\n",
    "            min_date = time_l\n",
    "        if max_date is None:\n",
    "            max_date = time_l\n",
    "            \n",
    "        # Update maximum and minimum dates\n",
    "        if min_date > time_l:\n",
    "            min_date = time_l\n",
    "        if max_date < time_l:\n",
    "            max_date = time_l\n",
    "            \n",
    "    return min_date, max_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date, max_date = check_same_year(datasets)\n",
    "print('Datasets are enclosed between {} and {}'.format(min_date, max_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data download\n",
    "\n",
    "Download datasets into the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url(url):\n",
    "    \"\"\" Extracts url content \"\"\"\n",
    "    response = requests.get(url.encode('latin-1'), stream=True)\n",
    "    response.raw.decode_content = True\n",
    "    if response.status_code == 200:\n",
    "        return response\n",
    "    else:\n",
    "        raise RuntimeError('Unexpected error: Response status from %s is %d') \n",
    "        % (url, response.status_code))\n",
    "\n",
    "\n",
    "def download_and_untar(folder, url, tmp_path):\n",
    "    \"\"\" Downloads data into temporary file and uncompress it into the given path \"\"\"\n",
    "    # Requests data from URL\n",
    "    response = download_url(url)\n",
    "    \n",
    "    # Set destination folder\n",
    "    dst = os.path.join(folder, os.path.basename(url)[:-3])\n",
    "    \n",
    "    # Create temporary file\n",
    "    fd, downloaded = tempfile.mkstemp(suffix='.csv.gz')\n",
    "\n",
    "    # Copy gz into tmp location\n",
    "    with open(downloaded, 'wb') as f: \n",
    "        shutil.copyfileobj(response.raw, f)\n",
    "\n",
    "    # Unzip into destination\n",
    "    with gzip.open(downloaded, 'rb') as f_in, open(dst, 'wb') as f_out:\n",
    "        f_out.write(f_in.read())\n",
    "\n",
    "    # Close and remove temporary file\n",
    "    os.close(fd)\n",
    "    os.remove(downloaded)\n",
    "            \n",
    "        \n",
    "def download_file(folder, url):\n",
    "    \"\"\" Extracts url content into given folder \"\"\"\n",
    "    # Requests data from URL\n",
    "    response = download_url(url)\n",
    "    # Otherwise, copy into destination\n",
    "    dst = os.path.join(folder, os.path.basename(url))\n",
    "    with open(dst, 'wb') as f: \n",
    "        shutil.copyfileobj(response.raw, f)\n",
    "\n",
    "        \n",
    "def get_info_path(folder):\n",
    "    return os.path.join(folder, 'info.dat')\n",
    "    \n",
    "\n",
    "def store_metadata(folder, city, state, country, time):\n",
    "    \"\"\" Stores timestamp into the given folder \"\"\"\n",
    "    with open(get_info_path(folder), 'wb') as f:\n",
    "        metadata = {'time': time, 'city': city, 'state': state, 'country':country}\n",
    "        pickle.dump(metadata, f)\n",
    "    \n",
    "\n",
    "def read_timestamp(folder):\n",
    "    \"\"\" Reads timestamp for the given dataset \"\"\"\n",
    "    with open(get_info_path(folder), 'rb') as f:\n",
    "        return pickle.load(f)['time']\n",
    "    \n",
    "\n",
    "# Create root folder\n",
    "airbnb_tmp_root = get_tmp_data_location(Datasets.AIRBNB_PRICE)\n",
    "create_dir(airbnb_tmp_root)\n",
    "\n",
    "# Create temporary file\n",
    "tmp_folder = tempfile.mkdtemp()\n",
    "\n",
    "# Create subfolder for each city\n",
    "for c in tqdm(datasets.keys(), desc='Downloading Airbnb city datasets ...'):\n",
    "    \n",
    "    # Create directory for city\n",
    "    city_folder = os.path.join(airbnb_tmp_root, c)\n",
    "    download = False\n",
    "    \n",
    "    # Download dataset if one found is more recent or if not found\n",
    "    if os.path.exists(city_folder):\n",
    "        current_timestamp = read_timestamp(city_folder)\n",
    "        if datasets[c]['timestamp'] < current_timestamp:\n",
    "            download = True\n",
    "    else:\n",
    "        create_dir(city_folder)\n",
    "        download = True\n",
    "\n",
    "    if download:\n",
    "        # Download compressed files for file\n",
    "        download_and_untar(city_folder, datasets[c]['listings'], tmp_folder)\n",
    "        download_and_untar(city_folder, datasets[c]['calendar'], tmp_folder)\n",
    "        download_and_untar(city_folder, datasets[c]['reviews'], tmp_folder)\n",
    "        download_file(city_folder, datasets[c]['neighbourhoods'])\n",
    "    \n",
    "        # Store timestamp\n",
    "        state, country, time = datasets[c]['state'], datasets[c]['country'], datasets[c]['timestamp']\n",
    "        store_metadata(city_folder, c, state, country, time)\n",
    "    \n",
    "# Clean temp folder\n",
    "shutil.rmtree(tmp_folder)\n",
    "print('Downloaded data for %s cities' % len(datasets.keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
